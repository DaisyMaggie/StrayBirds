---
layout: default
title: Text Retrieval and Search Engines 4
comments: false
---

#### What are some of the general challenges in building a web search engine?
  
Scalability challenges
  
   * How to handle the size of the web and ensure completeness of coverage?
   * How to serve many user queries quickly?

Low quality information and spams
New pages are constantly created and soem pages may be updated very quickly


#### What is a crawler? How can we implement a simple crawler?
A crawler/spider/robot is a program that visits web sites and reads their pages and other information in order to create entries for a search engine index.

Building a simple crawler

  * Start with a set of "seed pages" in a priority queue
  * Fetch pages from the web
  * Parse fetched pages for hyperlinks; add them to the queue
  * Follow the hyperlinks in the queue
 

#### What is focused crawling? What is incremental crawling?

Focused crawling is to crawl just some pages about a particular topic(e.g., all pages about "automobiles").

Incremental/repeated crawling  is to crawl the pages which has been modified after last successful crawl.

#### What kind of pages should have a higher priority for recrawling in incremental crawling?

Frequently updated pages
Frequently accessed pages

#### What can we do if the inverted index doesn’t fit in any single machine?

Store the data in multiple machines and process the data in parallel.

#### What’s the basic idea of Google File System (GFS)?

The GFS master works as the simple centrallized management of the file system. The GFS client obtains the chunk’s handle and locations from the GFS master and then obtains the actual file data directly from one of the GFS chunkservers without involving other nodes.

#### How does MapReduce work? What are the two key functions that a programmer needs to implement when programming with a MapReduce framework? 

The input data is separated into multiple key/value pairs; each key/value pair is assigned to one of the map functions running parallelly; each map function processes the ke/value pairs assigned and generate new key/values pairs; internal collection/sorting function process the key/values pairs generated by map functions on the keys so that all  data beloning to one key is grouped in one key/value pair; these key/value pairs are sent to reduce function to generate the output.  

#### How can we use MapReduce to build an inverted index in parallel?

* Map funtion:
  
  Input: key denotes the document id; value denotes the words in the document
  Output: a word in the document as key; count of the word in the document plus document id as value
  

#### What is anchor text? Why is it useful for improving search accuracy?
#### What is a hub page? What is an authority page?
#### What kind of web pages tend to receive high scores from PageRank?
#### How can we interpret PageRank from the perspective of a random surfer “walking” on the web?
#### How exactly do you compute PageRank scores?
#### How does the HITS algorithm work?
#### What’s the basic idea of learning to rank?
#### How can logistic regression be used to combine multiple features for improving ranking accuracy of a search engine?
#### What is content-based information filtering?
#### How can we use a linear utility function to evaluate a filtering system? How should we set the coefficients in such a linear utility function?
#### How can we extend a retrieval system to perform content-based information filtering?
#### What is exploration-exploitation tradeoff?
#### How does the beta-gamma threshold learning algorithm work?
#### What is the basic idea of collaborative filtering?
#### How does the memory-based collaborative filtering algorithm work?
#### What is the “cold start” problem in collaborative filtering?
