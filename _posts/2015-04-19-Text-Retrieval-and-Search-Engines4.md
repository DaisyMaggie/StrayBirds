---
layout: default
title: Text Retrieval and Search Engines 4
comments: false
---

#### What are some of the general challenges in building a web search engine?
  
Scalability challenges
  
   * How to handle the size of the web and ensure completeness of coverage?
   * How to serve many user queries quickly?

Low quality information and spams

New pages are constantly created and some pages may be updated very quickly


#### What is a crawler? How can we implement a simple crawler?
A crawler/spider/robot is a program that visits web sites and reads their pages and other information in order to create entries for a search engine index.

Building a simple crawler

  * Start with a set of "seed pages" in a priority queue
  * Fetch pages from the web
  * Parse fetched pages for hyperlinks; add them to the queue
  * Follow the hyperlinks in the queue
 

#### What is focused crawling? What is incremental crawling?

Focused crawling is to crawl just some pages about a particular topic(e.g., all pages about "automobiles").

Incremental/repeated crawling  is to crawl the pages which has been modified after last successful crawl.

#### What kind of pages should have a higher priority for recrawling in incremental crawling?

Frequently updated pages
Frequently accessed pages

#### What can we do if the inverted index doesn’t fit in any single machine?

Store the data in multiple machines and process the data in parallel.

#### What’s the basic idea of Google File System (GFS)?

The GFS master works as the simple centrallized management of the file system. The GFS client obtains the chunk’s handle and locations from the GFS master and then obtains the actual file data directly from one of the GFS chunkservers without involving other nodes.

#### How does MapReduce work? What are the two key functions that a programmer needs to implement when programming with a MapReduce framework? 

The input data is separated into multiple key/value pairs; each key/value pair is assigned to one of the map functions running parallelly; each map function processes the ke/value pairs assigned and generate new key/values pairs; internal collection/sorting function process the key/values pairs generated by map functions on the keys so that all  data beloning to one key is grouped in one key/value pair; these key/value pairs are sent to reduce function to generate the output.  

#### How can we use MapReduce to build an inverted index in parallel?

* Map funtion:
  
  Input: key denotes the document id; value denotes the words in the document
  
  Output: a word in the document as key; count of the word in the document plus document id as value
  
* Reduce function

  Input: a word as key; list of valus (D, count) D denotes the the if of the document which contain the word and count denotes the frequency of the word in the document
  
  reduce just concat the key/value pairs by key

#### What is anchor text? Why is it useful for improving search accuracy?

Anchor text is the description of a page a link points to. It is the visible, clickable text in a hyperlink. Anchor text is weighted (ranked) highly in search engine algorithms, because the linked text is usually relevant to the landing page.

#### What is a hub page? What is an authority page?

A hub page is a page linking to a lot of page.

An authority page is a page having a lot of incoming links.


#### What kind of web pages tend to receive high scores from PageRank?

Authority page 

#### How can we interpret PageRank from the perspective of a random surfer “walking” on the web?

PageRank score is the probability that a random surfer visits a particular page.

#### How exactly do you compute PageRank scores?

 * Picture the Web net as a directed graph, with nodes represented by web pages and edges represented by the links between them;
 
 * Create transition matrix A of the graph: each page should transfer evenly its importance to the pages that it links to;
 
 * Denote by v the initial rank vector: initially the importance is uniformly distributed among the nodes; 
 
 * Update the rank of each page by adding to the current value the importance of the incoming links. This is the same as multiplying the matrix A with v;
 
 * Repeat update until convergence.

#### How does the HITS algorithm work?

HITS makes use of the link structure of the web in order to discover and rank pages relevant for a particular topic. HITS identifies good authorities and hubs for a topic by assigning two numbers to a page: an authority and a hub weight. These weights are defined recursively. A higher authority weight occurs if the page is pointed to by pages with high hub weights. A higher hub weight occurs if the page points to many pages with high authority weights.

#### What’s the basic idea of learning to rank?

Using machine learning to combine different features to improve the ranking function.

#### How can logistic regression be used to combine multiple features for improving ranking accuracy of a search engine?



#### What is content-based information filtering?



#### How can we use a linear utility function to evaluate a filtering system? How should we set the coefficients in such a linear utility function?



#### How can we extend a retrieval system to perform content-based information filtering?
#### What is exploration-exploitation tradeoff?
#### How does the beta-gamma threshold learning algorithm work?
#### What is the basic idea of collaborative filtering?
#### How does the memory-based collaborative filtering algorithm work?
#### What is the “cold start” problem in collaborative filtering?
